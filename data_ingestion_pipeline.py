# -*- coding: utf-8 -*-
"""Data_ingestion_Pipeline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ryh6iHNKc2MPq8rVQaGj6M9bfXw4Dwsn

## Task:

* Take any csv/text file of 2+ GB of your choice. --- (You can do this assignment on Google colab)

* Read the file ( Present approach of reading the file )

* Try different methods of file reading eg: Dask, Modin, Ray, pandas and present your findings in term of computational     efficiency

* Perform basic validation on data columns : eg: remove special character , white spaces from the col name

* As you already know the schema hence create a YAML file and write the column name in YAML file. --define separator of   
  read and write file, column name in YAML

* Validate number of columns and column name of ingested file with YAML.

* Write the file in pipe separated text file (|) in gz format.

* Create a summary of the file:

    Total number of rows,

    total number of columns

    file size
    
# Data Ingestion


  > Create a utility file
  
  > Config file creation
  
  > Data ingestion pipeline
"""

from google.colab import drive
drive.mount('/content/drive')

import os

if not os.path.isdir('week_7'):
  os.makedirs('week_7')

cd week_7/

# Commented out IPython magic to ensure Python compatibility.
# %%writefile testutility.py
# import logging
# import os
# import subprocess
# import yaml
# import pandas as pd
# import datetime
# import gc
# import re
# 
# 
# ################
# # File Reading #
# ################
# 
# def read_config_file(filepath):
#     with open(filepath, 'r') as stream:
#         try:
#             return yaml.safe_load(stream)
#         except yaml.YAMLError as exc:
#             logging.error(exc)
# 
# 
# def replacer(string, char):
#     pattern = char + '{2,}'
#     string = re.sub(pattern, char, string)
#     return string
# 
# def col_header_val(df,table_config):
#     '''
#     replace whitespaces in the column
#     and standardized column names
#     '''
#     df.columns = df.columns.str.lower()
#     df.columns = df.columns.str.replace('[^\w]','_',regex=True)
#     df.columns = list(map(lambda x: x.strip('_'), list(df.columns)))
#     df.columns = list(map(lambda x: replacer(x,'_'), list(df.columns)))
#     expected_col = list(map(lambda x: x.lower(),  table_config['columns']))
#     expected_col.sort()
#     df.columns =list(map(lambda x: x.lower(), list(df.columns)))
#     df = df.reindex(sorted(df.columns), axis=1)
#     if len(df.columns) == len(expected_col) and list(expected_col)  == list(df.columns):
#         print("column name and column length validation passed")
#         return 1
#     else:
#         print("column name and column length validation failed")
#         mismatched_columns_file = list(set(df.columns).difference(expected_col))
#         print("Following File columns are not in the YAML file",mismatched_columns_file)
#         missing_YAML_file = list(set(expected_col).difference(df.columns))
#         print("Following YAML columns are not in the file uploaded",missing_YAML_file)
#         logging.info(f'df columns: {df.columns}')
#         logging.info(f'expected columns: {expected_col}')
#         return 0

"""### Write YAML file"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile file.yaml
# file_type: csv
# dataset_name: Skin_Cancer
# file_name: metadata
# table_name: full_data
# inbound_delimiter: ","
# outbound_delimiter: "|"
# skip_leading_rows: 1
# columns:
#     - diagnostic
#     - has_piped_water
#     - diameter_2
#     - diameter_1
#     - cancer_history
#     - itch
#     - smoke
#     - hurt
#     - img_id
#     - region
#     - fitspatrick
#     - background_mother
#     - gender
#     - elevation
#     - background_father
#     - grew
#     - lesion_id
#     - biopsed
#     - patient_id
#     - drink
#     - bleed
#     - has_sewage_system
#     - age
#     - pesticide
#     - skin_cancer_history
#     - changed

# Read config file
import testutility as util
config_data = util.read_config_file("file.yaml")

config_data['inbound_delimiter']

#inspecting data of config file
config_data

# Normal reading process of the file
import pandas as pd
df_sample = pd.read_csv("metadata.csv",delimiter=',')
df_sample.head()

# read the file using config file
file_type = config_data['file_type']
source_file = "./" + config_data['file_name'] + f'.{file_type}'
#print("",source_file)
df = pd.read_csv(source_file,config_data['inbound_delimiter'])
df.head()

#validate the header of the file
util.col_header_val(df,config_data)

print("columns of files are:" ,df.columns)
print("columns of YAML are:" ,config_data['columns'])

if util.col_header_val(df,config_data)==0:
    print("validation failed")
    # write code to reject the file
else:
    print("col validation passed")
    # write the code to perform further action
    # in the pipleine

pd.read_csv("./metadata.csv")

df

output_path = '/content/week_7/file.txt.gz'
df.to_csv(output_path, sep='|', compression='gzip', index=False)

# Summary
summary = {
    'Total number of rows': len(df),
    'Total number of columns': len(df.columns),
    'File size (bytes)': os.path.getsize(output_path)
}

print(summary)